{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Results Analysis\n",
        "\n",
        "Parse and visualize training results from SLURM log files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path().resolve().parent.parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 11\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required modules\n",
        "import re\n",
        "from typing import Dict\n",
        "\n",
        "# Define functions\n",
        "def parse_log_file(log_path: Path) -> Dict:\n",
        "    \"\"\"Parse SLURM log file and extract key metrics.\"\"\"\n",
        "    results = {\n",
        "        'job_id': None, 'node': None, 'start_time': None, 'end_time': None, 'duration': None,\n",
        "        'train_samples': None, 'val_samples': None, 'test_samples': None,\n",
        "        'train_class_dist': {}, 'val_class_dist': {}, 'test_class_dist': {},\n",
        "        'best_val_loss': None, 'test_loss': None, 'test_accuracy': None,\n",
        "        'test_icu_stays': None, 'epochs': [],\n",
        "    }\n",
        "    \n",
        "    if not log_path.exists():\n",
        "        print(f\"âŒ Log-Datei nicht gefunden: {log_path}\")\n",
        "        return results\n",
        "    \n",
        "    content = log_path.read_text()\n",
        "    \n",
        "    job_match = re.search(r'Job ID: (\\d+)', content)\n",
        "    if job_match:\n",
        "        results['job_id'] = job_match.group(1)\n",
        "    \n",
        "    node_match = re.search(r'Node: (\\S+)', content)\n",
        "    if node_match:\n",
        "        results['node'] = node_match.group(1)\n",
        "    \n",
        "    start_match = re.search(r'Start time: (.+)', content)\n",
        "    if start_match:\n",
        "        results['start_time'] = start_match.group(1)\n",
        "    \n",
        "    end_match = re.search(r'End time: (.+)', content)\n",
        "    if end_match:\n",
        "        results['end_time'] = end_match.group(1)\n",
        "    \n",
        "    duration_match = re.search(r'Job Wall-clock time: (.+)', content)\n",
        "    if duration_match:\n",
        "        results['duration'] = duration_match.group(1)\n",
        "    \n",
        "    train_match = re.search(r'ECGDataset Statistics:.*?Total samples: (\\d+)', content, re.DOTALL)\n",
        "    if train_match:\n",
        "        results['train_samples'] = int(train_match.group(1))\n",
        "    \n",
        "    class_dist_matches = re.findall(r'Class distribution \\(after filtering\\): (\\{[^}]+\\})', content)\n",
        "    if len(class_dist_matches) >= 1:\n",
        "        results['train_class_dist'] = eval(class_dist_matches[0])\n",
        "    if len(class_dist_matches) >= 2:\n",
        "        results['val_class_dist'] = eval(class_dist_matches[1])\n",
        "    if len(class_dist_matches) >= 3:\n",
        "        results['test_class_dist'] = eval(class_dist_matches[2])\n",
        "    \n",
        "    if results['train_class_dist']:\n",
        "        results['train_samples'] = sum(results['train_class_dist'].values())\n",
        "    if results['val_class_dist']:\n",
        "        results['val_samples'] = sum(results['val_class_dist'].values())\n",
        "    if results['test_class_dist']:\n",
        "        results['test_samples'] = sum(results['test_class_dist'].values())\n",
        "    \n",
        "    val_loss_match = re.search(r'Best validation loss: ([\\d.]+)', content)\n",
        "    if val_loss_match:\n",
        "        results['best_val_loss'] = float(val_loss_match.group(1))\n",
        "    \n",
        "    test_loss_match = re.search(r'Test Loss: ([\\d.]+)', content)\n",
        "    if test_loss_match:\n",
        "        results['test_loss'] = float(test_loss_match.group(1))\n",
        "    \n",
        "    test_acc_match = re.search(r'Test Accuracy:\\s+([\\d.]+)', content)\n",
        "    if test_acc_match:\n",
        "        results['test_accuracy'] = float(test_acc_match.group(1))\n",
        "    \n",
        "    test_balanced_acc_match = re.search(r'Balanced Accuracy:\\s*([\\d.]+)', content)\n",
        "    if test_balanced_acc_match:\n",
        "        results['test_balanced_accuracy'] = float(test_balanced_acc_match.group(1))\n",
        "    \n",
        "    test_macro_precision_match = re.search(r'Macro Precision:\\s*([\\d.]+)', content)\n",
        "    if test_macro_precision_match:\n",
        "        results['test_macro_precision'] = float(test_macro_precision_match.group(1))\n",
        "    \n",
        "    test_macro_recall_match = re.search(r'Macro Recall:\\s*([\\d.]+)', content)\n",
        "    if test_macro_recall_match:\n",
        "        results['test_macro_recall'] = float(test_macro_recall_match.group(1))\n",
        "    \n",
        "    test_macro_f1_match = re.search(r'Macro F1-Score:\\s*([\\d.]+)', content)\n",
        "    if test_macro_f1_match:\n",
        "        results['test_macro_f1'] = float(test_macro_f1_match.group(1))\n",
        "    \n",
        "    per_class_section = re.search(r'Per-Class Metrics:(.*?)(?:\\n\\s*\\n|Confusion Matrix:)', content, re.DOTALL)\n",
        "    if per_class_section:\n",
        "        per_class_lines = per_class_section.group(1).strip().split('\\n')\n",
        "        results['per_class_metrics'] = {}\n",
        "        for line in per_class_lines[2:]:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            if len(parts) >= 4 and parts[0].isdigit():\n",
        "                try:\n",
        "                    cls = int(parts[0])\n",
        "                    results['per_class_metrics'][cls] = {\n",
        "                        'precision': float(parts[1]),\n",
        "                        'recall': float(parts[2]),\n",
        "                        'f1': float(parts[3])\n",
        "                    }\n",
        "                except (ValueError, IndexError):\n",
        "                    continue\n",
        "    \n",
        "    cm_section = re.search(r'Confusion Matrix:\\s*\\n(.*?)(?:\\n\\s*\\n|End time|Job completed)', content, re.DOTALL)\n",
        "    if cm_section:\n",
        "        cm_lines = [line.strip() for line in cm_section.group(1).strip().split('\\n') if line.strip()]\n",
        "        if cm_lines:\n",
        "            results['confusion_matrix'] = []\n",
        "            for line in cm_lines:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                if line and line[0].isdigit():\n",
        "                    parts = line.split()\n",
        "                    if len(parts) > 1:\n",
        "                        try:\n",
        "                            row = [int(x) for x in parts[1:]]\n",
        "                            results['confusion_matrix'].append(row)\n",
        "                        except (ValueError, IndexError):\n",
        "                            continue\n",
        "    \n",
        "    test_stays_match = re.search(r'Number of ICU stays evaluated: (\\d+)', content)\n",
        "    if test_stays_match:\n",
        "        results['test_icu_stays'] = int(test_stays_match.group(1))\n",
        "    \n",
        "    epoch_matches = re.findall(r'Epoch (\\d+)/(\\d+).*?Train Loss: ([\\d.]+).*?Val Loss: ([\\d.]+)', content)\n",
        "    if epoch_matches:\n",
        "        results['epochs'] = [\n",
        "            {\n",
        "                'epoch': int(e[0]),\n",
        "                'total': int(e[1]),\n",
        "                'train_loss': float(e[2]),\n",
        "                'val_loss': float(e[3])\n",
        "            }\n",
        "            for e in epoch_matches[-5:]\n",
        "        ]\n",
        "    \n",
        "    return results\n",
        "\n",
        "def format_class_distribution(dist: Dict[int, int]) -> str:\n",
        "    \"\"\"Format class distribution as a compact string.\"\"\"\n",
        "    if not dist:\n",
        "        return \"N/A\"\n",
        "    total = sum(dist.values())\n",
        "    percentages = {k: (v / total * 100) for k, v in dist.items()}\n",
        "    return f\"Total: {total} | \" + \" | \".join([f\"C{k}: {v} ({p:.1f}%)\" for k, v, p in \n",
        "                                               sorted([(k, v, percentages[k]) for k, v in dist.items()])])\n",
        "\n",
        "def print_results(results: Dict):\n",
        "    \"\"\"Print results in a formatted way.\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"ðŸ“Š TRAINING RESULTS SUMMARY\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    print(\"\\nðŸ”¹ Job Information:\")\n",
        "    print(f\"   Job ID:      {results['job_id'] or 'N/A'}\")\n",
        "    print(f\"   Node:        {results['node'] or 'N/A'}\")\n",
        "    print(f\"   Start:       {results['start_time'] or 'N/A'}\")\n",
        "    print(f\"   End:         {results['end_time'] or 'N/A'}\")\n",
        "    print(f\"   Duration:    {results['duration'] or 'N/A'}\")\n",
        "    \n",
        "    print(\"\\nðŸ”¹ Dataset Split:\")\n",
        "    print(f\"   Train:       {results['train_samples'] or 'N/A':,} samples\")\n",
        "    print(f\"   Validation:  {results['val_samples'] or 'N/A':,} samples\")\n",
        "    print(f\"   Test:        {results['test_samples'] or 'N/A':,} samples\")\n",
        "    total = (results['train_samples'] or 0) + (results['val_samples'] or 0) + (results['test_samples'] or 0)\n",
        "    print(f\"   Total:       {total:,} samples\")\n",
        "    \n",
        "    print(\"\\nðŸ”¹ Class Distribution:\")\n",
        "    print(f\"   Train:       {format_class_distribution(results['train_class_dist'])}\")\n",
        "    print(f\"   Validation:  {format_class_distribution(results['val_class_dist'])}\")\n",
        "    print(f\"   Test:        {format_class_distribution(results['test_class_dist'])}\")\n",
        "    \n",
        "    print(\"\\nðŸ”¹ Model Performance:\")\n",
        "    if results['best_val_loss'] is not None:\n",
        "        print(f\"   Best Validation Loss: {results['best_val_loss']:.4f}\")\n",
        "    if results['test_loss'] is not None:\n",
        "        print(f\"   Test Loss:            {results['test_loss']:.4f}\")\n",
        "    if results['test_accuracy'] is not None:\n",
        "        print(f\"   Test Accuracy:        {results['test_accuracy']:.4f} ({results['test_accuracy']*100:.2f}%)\")\n",
        "    if results.get('test_balanced_accuracy') is not None:\n",
        "        print(f\"   Balanced Accuracy:    {results['test_balanced_accuracy']:.4f} ({results['test_balanced_accuracy']*100:.2f}%)\")\n",
        "    if results.get('test_macro_precision') is not None:\n",
        "        print(f\"   Macro Precision:      {results['test_macro_precision']:.4f}\")\n",
        "    if results.get('test_macro_recall') is not None:\n",
        "        print(f\"   Macro Recall:         {results['test_macro_recall']:.4f}\")\n",
        "    if results.get('test_macro_f1') is not None:\n",
        "        print(f\"   Macro F1-Score:       {results['test_macro_f1']:.4f}\")\n",
        "    if results['test_icu_stays'] is not None:\n",
        "        print(f\"   Test ICU Stays:       {results['test_icu_stays']:,}\")\n",
        "    \n",
        "    if results.get('per_class_metrics'):\n",
        "        print(\"\\nðŸ”¹ Per-Class Metrics:\")\n",
        "        print(f\"   {'Class':<8} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
        "        print(\"   \" + \"-\" * 44)\n",
        "        for cls in sorted(results['per_class_metrics'].keys()):\n",
        "            metrics = results['per_class_metrics'][cls]\n",
        "            print(f\"   {cls:<8} {metrics['precision']:<12.4f} {metrics['recall']:<12.4f} {metrics['f1']:<12.4f}\")\n",
        "    \n",
        "    if results.get('confusion_matrix'):\n",
        "        print(\"\\nðŸ”¹ Confusion Matrix:\")\n",
        "        cm = results['confusion_matrix']\n",
        "        if cm and len(cm) > 0:\n",
        "            num_rows = len(cm)\n",
        "            num_cols = len(cm[0]) if cm[0] else 0\n",
        "            num_classes = max(num_rows, num_cols)\n",
        "            print(\"   \" + \" \".join([f\"{i:>6}\" for i in range(num_classes)]))\n",
        "            for i in range(num_classes):\n",
        "                if i < num_rows and len(cm[i]) >= num_classes:\n",
        "                    row_str = f\"   {i} \" + \" \".join([f\"{cm[i][j]:>6}\" for j in range(num_classes)])\n",
        "                    print(row_str)\n",
        "                elif i < num_rows:\n",
        "                    row = cm[i] + [0] * (num_classes - len(cm[i]))\n",
        "                    row_str = f\"   {i} \" + \" \".join([f\"{row[j]:>6}\" for j in range(num_classes)])\n",
        "                    print(row_str)\n",
        "                else:\n",
        "                    row_str = f\"   {i} \" + \" \".join([\"     0\"] * num_classes)\n",
        "                    print(row_str)\n",
        "    \n",
        "    if results['epochs']:\n",
        "        print(\"\\nðŸ”¹ Last 5 Epochs:\")\n",
        "        print(f\"   {'Epoch':<8} {'Train Loss':<12} {'Val Loss':<12}\")\n",
        "        print(\"   \" + \"-\" * 32)\n",
        "        for ep in results['epochs']:\n",
        "            print(f\"   {ep['epoch']:<8} {ep['train_loss']:<12.4f} {ep['val_loss']:<12.4f}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    if results.get('job_id'):\n",
        "        checkpoint_path = f\"outputs/checkpoints/CNNScratch_best_{results['job_id']}.pt\"\n",
        "        print(f\"âœ… Checkpoints: {checkpoint_path}\")\n",
        "    else:\n",
        "        print(\"âœ… Checkpoints: outputs/checkpoints/CNNScratch_best_<JOB_ID>.pt (Job ID not found)\")\n",
        "    print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify job ID or use latest\n",
        "job_id = None  # Set to specific job ID, or leave None to use latest\n",
        "\n",
        "if job_id is None:\n",
        "    # Find latest log file\n",
        "    log_dir = Path(\"outputs/logs\")\n",
        "    if log_dir.exists():\n",
        "        log_files = sorted(log_dir.glob(\"slurm_*.out\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "        if log_files:\n",
        "            job_id = log_files[0].stem.replace(\"slurm_\", \"\")\n",
        "            print(f\"Using latest log file: {log_files[0].name}\")\n",
        "        else:\n",
        "            print(\"No log files found!\")\n",
        "    else:\n",
        "        print(\"Log directory not found!\")\n",
        "else:\n",
        "    print(f\"Using job ID: {job_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse and display results\n",
        "if job_id:\n",
        "    log_path = Path(f\"outputs/logs/slurm_{job_id}.out\")\n",
        "    \n",
        "    if log_path.exists():\n",
        "        results = parse_log_file(log_path)\n",
        "        print_results(results)\n",
        "        \n",
        "        # Visualize training history if available\n",
        "        if results.get('epochs'):\n",
        "            epochs_data = results['epochs']\n",
        "            epochs = [e['epoch'] for e in epochs_data]\n",
        "            train_losses = [e['train_loss'] for e in epochs_data]\n",
        "            val_losses = [e['val_loss'] for e in epochs_data]\n",
        "            \n",
        "            fig, ax = plt.subplots(figsize=(12, 6))\n",
        "            ax.plot(epochs, train_losses, label='Train Loss', marker='o')\n",
        "            ax.plot(epochs, val_losses, label='Val Loss', marker='s')\n",
        "            ax.set_xlabel('Epoch', fontsize=12)\n",
        "            ax.set_ylabel('Loss', fontsize=12)\n",
        "            ax.set_title('Training History', fontsize=14, fontweight='bold')\n",
        "            ax.legend()\n",
        "            ax.grid(alpha=0.3)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "    else:\n",
        "        print(f\"Log file not found: {log_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
