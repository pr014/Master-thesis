# Baseline configuration for 24h ECG dataset with class weights (SQRT method)
# This config is specifically for the preprocessed_24h_1 dataset
# Class weights are calculated using Square-Root Weighting (SQINV) method
# 
# IMPORTANT: This config is dataset-specific. Do not use for other datasets!
#
# Weighting Method: Square-Root Weighting (SQINV)
# Formula: w_i = sqrt(n_total) / sqrt(n_i), then normalized to mean = 1.0
# Rationale: Square-root of inverse-frequency flattens extreme weights and
#            avoids overfitting on rare classes (common in medical/CV studies)

# Random seed for reproducibility
seed: 42

# Device settings
device:
  device: null  # null = auto-select (CUDA if available, else CPU)
  deterministic: false

# Data settings for 24h dataset
data:
  data_dir: "/home/ka/ka_aifb/ka_zx9981/workspace/ma-thesis/MA-thesis-1/preprocessed_24h_1"
  sampling_rate: 500.0  # Hz
  window_seconds: 10.0  # seconds
  num_leads: 12  # 12-lead ECG
  
  # Preprocessing (same as baseline)
  preprocessing:
    target_sampling_rate: 500.0
    window_seconds: 10.0
    filter_type: "bandpass"
    filter_lowcut: 0.5
    filter_highcut: 50.0
    filter_order: 4
    normalize: "zscore"
  
  # Data augmentation (same as baseline)
  augmentation:
    enabled: true
    gaussian_noise: true
    noise_std: 0.03
    amplitude_scaling: true
    scale_min: 0.85
    scale_max: 1.15

# Training hyperparameters
training:
  batch_size: 64
  num_epochs: 50
  num_workers: 4
  pin_memory: true
  
  # Optimizer
  optimizer:
    type: "Adam"
    lr: 5e-4
    weight_decay: 1e-4
    betas: [0.9, 0.999]
  
  # Gradient clipping
  gradient_clip_norm: 1.0
  
  # Dropout rate
  dropout_rate: 0.3
  
  # Learning rate scheduler
  scheduler:
    type: "ReduceLROnPlateau"
    factor: 0.1
    patience: 5
    min_lr: 1e-6
  
  # Loss function with class weights for 24h dataset (SQRT method)
  # Weights calculated based on training set distribution (28,147 samples):
  # Class distribution: [4367, 9294, 4985, 2848, 1781, 1063, 808, 506, 448, 2047]
  # Method: Square-Root Weighting (SQINV)
  #   Formula: w_i = sqrt(n_total) / sqrt(n_i)
  #   Then normalized to mean = 1.0
  # This method flattens extreme weights compared to balanced weighting
  loss:
    type: "weighted_ce"
    weight: [1.5, 0.8, 1.1, 1.4, 1.8, 2.3, 2.7, 3.4, 3.6, 1.7]
    # Class 0: 1.5  (15.5% of samples) - was 2.1 with balanced
    # Class 1: 0.8  (33.0% of samples) - was 0.6 with balanced
    # Class 2: 1.1  (17.7% of samples) - was 1.1 with balanced
    # Class 3: 1.4  (10.1% of samples) - was 1.9 with balanced
    # Class 4: 1.8  (6.3% of samples)  - was 3.0 with balanced
    # Class 5: 2.3  (3.8% of samples)  - was 5.0 with balanced
    # Class 6: 2.7  (2.9% of samples)  - was 6.6 with balanced
    # Class 7: 3.4  (1.8% of samples)  - was 10.5 with balanced (reduced from extreme)
    # Class 8: 3.6  (1.6% of samples)  - was 11.8 with balanced (reduced from extreme)
    # Class 9: 1.7  (7.3% of samples)  - was 2.6 with balanced
    # 
    # Comparison: SQRT method reduces weight range from [0.6, 11.8] to [0.8, 3.6]
    # This should prevent the model from being too conservative and improve
    # learning of minority classes without extreme overfitting

# Validation settings
validation:
  val_split: 0.1  # 10% for 80-10-10 split
  val_frequency: 1

# Test settings
test_split: 0.1  # 10% for 80-10-10 split

# Early stopping
early_stopping:
  enabled: true
  patience: 7
  monitor: "val_loss"
  mode: "min"

# Checkpointing
checkpoint:
  save_dir: "outputs/checkpoints"
  save_frequency: 1
  save_best: true
  metric: "val_loss"

# Logging
logging:
  log_dir: "outputs/logs"
  use_tensorboard: true
  log_frequency: 10

# Evaluation metrics
evaluation:
  metrics:
    - "accuracy"
    - "f1_score"
    - "precision"
    - "recall"
    - "auc_roc"
    - "auc_pr"

